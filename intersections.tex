\chapter{Intersection Types generation}
\label{chp:intersection_generate}

As described in Section \ref{sct:dialyzer_preliminaries}, \dr\ has two
distinct phases in its analysis: during the first it calculates the
success typings of all the functions and during the second it finds
the discrepancies in their use. In this chapter the calculation of
intersectioned success typings will be presented, leaving their usage
in discrepancy detection for Chapter \ref{chp:intersection_usage}.

\section{Original type system and analysis}

Before describing the design and implementation of the intersection
types as well as the analysis needed to produce and use them, a brief
overview of the existing \er's type system will be given, focusing on
the type of functions and the analysis performed by \dr\ to generate
them. Especially for the analysis, \cite{Elli} explains everything in
far greater detail and is a suggested reading.

\subsection{Type system}

\er's type system includes types for all the basic term sets. Table
\ref{tab:erl_terms} briefly describes these sets and Table
\ref{tab:erl_types} contain the most commonly used \er\ types. More
information on these can be found in \cite{erlman, type_system, Manolis}.

\begin{fulltable}{|c|c|c|}{Categories of Erlang terms}{tab:erl_terms}
\hline
Category & Description & Examples \\
\hline
\hline
Integer & A mathematical integer & $-31$, $0$, $17$, $42$ \\
\hline
Float & A floating point number & $-0.123$, $3.14$ \\
\hline
Atom & A named constant & hello, `World' \\
\hline
Binary & An untyped series of bytes & <<255,0,98>>, <<42>> \\
\hline
Bitstring & An untyped series of bits & <<99,3:2>>, <<1:1,0:1>>, <<4>> \\
\hline
Pid & A handle for talking to an Erlang process & -- \\ % <0.36.0>
\hline
Port & A handle for talking to an external program & -- \\ % \#Port<0.51>
\hline
Reference & A term unique within a runtime environment & -- \\ % \#Ref<0.0.0.62>
\hline
\multirow{2}{*}{Fun} & \multirow{2}{*}{A callable function object}
& fun(X) $\rightarrow$ X + 1 end, \\ && fun lists:reverse/1 \\
\hline
\multirow{2}{*}{Tuple}
    & A compound term with       & \{0,alabama,3.14\}, \\
    & a fixed number of elements & \{answer,42\} \\
\hline
\multirow{2}{*}{List}
    & A compound term with a variable number of   & [1,2,3], [42,answer], \\
    & elements (not necessarily of the same type) & [for,whom,the,bell,tolls] \\
\hline
\end{fulltable}

\begin{fulltable}{|c|c|m{7.7cm}|}{Built-in Erlang types}{tab:erl_types}
\hline
Term Group & Related Types & Represented Terms \\
\hline
\hline
\multirow{5}{*}{Integers}
    & \emph{<Int>} & only a specific integer, \emph{<Int>} (singleton type) \\
    \cline{2-3}
    & \emph{<Lo>}$..$\emph{<Hi>}
    & integers between \emph{<Lo>} and \emph{<Hi>} \\
    \cline{2-3}
    & integer() & all integers \\
    \cline{2-3}
    & non\_neg\_integer() & non-negative integers \\
    \cline{2-3}
    & pos\_integer() & positive integers \\
    \cline{2-3}
    & neg\_integer() & negative integers \\
\hline
Floats & float() & all floats \\
\hline
\multirow{2}{*}{Atoms}
    & \emph{<Atom>} & only a specific atom, \emph{<Atom>} (singleton type) \\
    \cline{2-3}
    & atom() & all atoms \\
\hline
\multirow{3}{*}{Binaries}
    & binary() & all binaries \\
    \cline{2-3}
    & <<>> & only the empty binary (singleton type) \\
    \cline{2-3}
    & <<\_:\emph{<Base>}>> & binaries of length \emph{<Base>} (in bytes) \\
\hline
\multirow{4}{*}{Bitstrings}
    & bitstring() & all bitstrings \\
    \cline{2-3}
    & <<>> & only the empty bitstring (singleton type) \\
    \cline{2-3}
    & <<\_:\_*\emph{<Unit>}>>
    & bitstrings of length $k\times$\emph{<Unit>} (in bits) \\
    \cline{2-3}
    & <<\_:\emph{<B>}, \_:\_*\emph{<U>}>>
    & bitstrings of length \emph{<B>}$\times$\emph{<U>} (in bits) \\
\hline
Pids & pid() & all pids \\
\hline
Ports & port() & all ports \\
\hline
References & reference() & all references \\
\hline
\multirow{4}{*}{Funs}
    & fun() & all functions \\
    \cline{2-3}
    & fun((\ldots) $\rightarrow$ \emph{Type})
    & functions of any arity returning \emph{Type} \\
    \cline{2-3}
    & fun(() $\rightarrow$ \emph{Type})
    & zero-arity functions returning \emph{Type} \\
    \cline{2-3}
    & fun(($T_1$,\ldots,$T_N$) $\rightarrow$ \emph{R})
    & N-arity functions accepting arguments of types $T_1$,\ldots,$T_N$ and
      returning \emph{R} \\
\hline
\multirow{3}{*}{Tuples}
    & tuple() & all tuples \\
    \cline{2-3}
    & \{\} & only the zero-size tuple (singleton type) \\
    \cline{2-3}
    & \{$Type_1$,\ldots,$Type_N$\}
    & tuples of N elements, of types $Type_1$,\ldots,$Type_N$ \\
\hline
\multirow{3}{*}{Lists}
    & [] & only the empty list (singleton type) \\
    \cline{2-3}
    & [\emph{Type}] & lists with elements of type \emph{Type} \\
    \cline{2-3}
    & [\emph{Type},\ldots]
    & non-empty lists with elements of type \emph{Type} \\
\hline
\multirow{3}{*}{---}
    & any() & all Erlang terms \\
    \cline{2-3}
    & none() & no terms (special type) \\
    \cline{2-3}
    & $T_1$ | $T_2$ | \ldots\ | $T_N$
    & the union of all terms represented by $T_1$, $T_2$, \ldots, or $T_N$ \\
\hline
\end{fulltable}

\subsubsection{Function type}
\label{sct:orig_fun_type}

Some special attention should be given to the form of the function
type. As shown in Table \ref{tab:erl_types}, a function's type
consists of two parts:
\begin{enumerate}
\item The first part describes the type of the function
  arguments. This can be either a product of specific length, with one
  type for each of the function's arguments, or the type \emph{any} if
  we have no information about the number of arguments.
\item The second part describes the return type of the function. This
  can be a regular type or the special type \emph{unit} for functions
  that are not supposed return\footnote{This is the case for example
    in a function that implements a server's main loop}.
\end{enumerate}

\subsubsection{Type operations}
\label{sct:orig_type_operations}

Working with types requires special operators. The most important of
them will be presented now (as these were the ones which were mainly
affected by the introduction of intersection types), along with the
particular usage of them on function types.
\begin{description}
\item[Supremum:] The supremum of two types is the smallest type that
  contains them both. \er\ supports union types as shown in Table
  \ref{tab:erl_types} as well as special unions for common types (like
  atoms, integers and tuples). \dr\ may overapproximate a union type
  in cases where detail exceeds a certain level to keep the success
  typings analysis efficient. Such is the case with large sets of
  atoms for example: the type of the single-character atoms
  corresponding to the lowercase letters of the English alphabet is
  \atom\ and not \emph{a | b | ... | z}.

  The original calculation of two function types' supremum is the main
  reason for our inability to detect errors such as the one presented
  in Section \ref{sct:intersection_preliminaries}. As the type system
  has one field for the success typings of the arguments and one for
  the function's return, supremum simply creates the union of the
  arguments and the result and stores them in the respective
  fields. This produces wider success typings than desired (in the
  example shown in Listing \ref{lst:fun_sup_example} a function that
  takes an \atom\ and returns a \emph{number} is included in the type,
  even though none of the original members includes it). If the
  functions have different arity the domain type collapses into
  \emph{any}.

\begin{console}{lst:fun_sup_example}{Original function supremum example}
Supremum:
Type A : fun((atom())   -> atom())
Type B : fun((number()) -> number())
Result : fun((atom() | number()) -> atom() | number())
\end{console}

\item[Infimum:] The infimum of two types is the biggest type that is
  contained in both.

  In function types of the same arity this means that the infimum will
  have the infimum type for each argument and the infimum of the
  return types.

\item[Equality:] As there are no aliases two types are equal if and
  only if they are syntactically equal. This applies even to the
  internal representation which keeps every particular set (like
  \atoms\ or \emph{integers}), as well as mixed unions, ordered.

  Functions types are no exception and should also be syntactically
  equal to be equal.

\item[Is Subtype:] This operation is broken down to the calculation of
  the infimum of the two types and the check for its equality with the
  subtype candidate.
\end{description}

There are also special operations for function types:

\begin{description}
\item[Function domain:] Returns a list of types, one for each of the
  function's arguments.
\item[Function range:] Returns the range of the function.
\end{description}

\subsection{Original success typing analysis}
\label{sct:orig_analysis}

\dr\ begins the analysis by finding the calls between functions and
creating the respective callgraph. From this callgraph a partial
ordering of the functions is obtained and all functions' success
typings are calculated beginning from those who have no calls and
building on top of them.

The success typings analysis assigns a type variable to each of the
code's original variables and stores a mapping from each variable to a
type. Functions get two variables, one primary and a second one to be
used in self-calls and SCC calls. After that the code is traversed to
generate constraints and subsequently these are processed to obtain
the final type.

A brief presentation of the main types of constraints and the
processing algorithm will be included here to make illustrating the
differences introduced in the new analysis easier. For further details
you can refer to the appropriate sections in the bibliograhy
\cite{Elli, SuccessTypings@PPDP-06}.

\subsubsection{Constraints}
\label{sct:orig_constraints}

The constraints belong to one of the following kinds:
\begin{description}
\item[Simple constraints:] The simplest form of constraint states that
  a certain type should be equal to another or subtype of
  another. This is a natural requirement for function arguments for
  example, which must be subtypes of the corresponding success typing,
  calculated earlier in the analysis.
\item[Conjunctive lists:] The constraints generated from subsequent
  statements are stored in conjunctive lists as all must be satisfied
  at the same time. In simple functions the final constraint might be
  a conjunctive constraint list with simple constraints as elements.
\item[Disjunctive lists:] When branches of any kind are present in the
  code, each side of the branch generates a conjunctive list and all
  these lists are combined under a disjunction. After processing each
  of the conjunctions, the types for the disjunction are calculated by
  getting the supremum of the types for each variable on each branch.
\item[Constraint references:] Funs without name generate these. These
  are special and not really relevant with this extension so they are
  simply mentioned for completeness.
\end{description}

\begin{console}{lst:constraints}{Constraint examples}
%%Sample code

bar(1) -> 5;
bar(2) -> 10.

foo(a) -> b;
foo(X) ->
  Y = bar(X),
  Y*X.

%% Supposing we have alredy found the success typing:
%% bar(1 | 2) -> 5 | 10
%% The constraints for foo are:

Conjunctive List 1:                <- All the constraints for foo
 * var(1) eq fun(var(2)) -> var(3) <- Tying foo type to it's args and ret
 * Disjunctive List 2:             <- Due to the two clauses
 *  * Conjunctive List 3:          <- Constraints for the first clause
 *  *  * var(2) eq a
 *  *  * var(3) eq b
 *  * Conjunctive List 4:          <- Constraints for the second clause
 *  *  * var(2) sub 1|2            <- X (var(2)) is used as argument of bar
 *  *  * var(4) sub 5|10           <- A hidden variable (var(4)) for the result
 *  *  * var(5) eq var(4)          <- Assign the result to Y (var(5))
 ...
\end{console}

In Listing \ref{lst:constraints} a real example is given. As you can
see, the constraints of each function are collected in a main
conjunctive constraint list. In this list there exist some notable
constraints:

\begin{enumerate}
  \item \textbf{Generic function constraint:} This constraint has the
    form of the first element in the conjunctive list 1 of Listing
    \ref{lst:constraints}. It's purpose is to bind the function's type
    variable to the ones of the arguments and the result. In the
    example \emph{var(1)} is the type variable of a function with one
    argument (with type variable \emph{var(2)}) whose return type is
    \emph{var(3)}. This constraint is the actual constructor of the
    function's type.
  \item \textbf{Refined function constraint:} This constraint comes
    from the dataflow analysis and restricts the whole type of
    unexported functions according to the actual calls that are
    present within the module. For more information on function
    refinement see \ref{sct:dialyzer_preliminaries}. This constraint
    is omitted when the function is exported or dataflow has not yet
    been performed.
  \item \textbf{Constraints from clauses:} If the function has
    clauses, the third constraint in the list is a disjunctive list of
    the constraints generated in each of them (an example is the
    disjunctive list 2 in \ref{lst:constraints}). If the function has
    only one clause the constraints of it are added in the main
    conjunctive list as is.
\end{enumerate}

The previous constraints are present in the form described above in
every main conjunctive list. Some other forms of constraints that are
present in almost every function are these:

\begin{enumerate}
  \item \textbf{Branches:} Branches such as \emph{case} statements
    generate disjunctive lists, just like clauses do.
  \item \textbf{Function calls:} These produce a conjunctive list,
    requiring both the result's and the actual parameters' type
    variables to be subtypes of the respective success types.
  \item \textbf{Self and SCC calls:} These are treated specially:
    Initially these calls are supposed to fail. On subsequent
    iterations the types calculated in the previous step are used to
    extend the types of the argument and the result. In this way we
    begin to extract the type from clauses that are sure to return and
    build on top of them to find wider success typings. The
    overapproximations mentioned in \ref{sct:orig_type_operations}
    (Supremum operator) make this procedure efficient, as after a
    certain limit the types collapse into generic ones.
\end{enumerate}

\subsubsection{Processing}
\label{sct:orig_processing}

Processing is a fixpoint procedure when it comes to anything but
simple constraints. The latter simply restrain any variables they
contain according to the operation they contain (equality or subtype)
and store the result in the mapping. Lists and refs store the old
mappings and compare the new ones against them to find a fixpoint as
each element may affect others. Self-recursive functions and SCCs also
have special treatment as the previously calculated types are fed back
in to be further processed in the self or scc-related calls.

\section{Intersection types}
\label{sct:intersection_types}

In order to generate and use intersection types changes were required
in both the type system and \dr's analysis.

\subsection{Changes in the type system}
\label{sct:intersection_type_system}

\subsubsection{Representation}
\label{sct:intersection_representation}

We need the ability to store multiple domains with the respective
ranges. Therefore we will substitute the original two fields in the
function type with a list of tuples of arity 2. Each tuple will
contain a domain and a range and will be referred as a \emph{clause}.
\dr\ took advantage of the simple old form and stored the type of the
function in the plt using a tuple containing the two old parts. This
had to be changed and a proper full function type to be stored
instead.

\subsubsection{Semantics}
\label{sct:intersection_semantics}

For simplicity the order of the clauses will have no special
meaning. This is the main difference with the ordinary \er's function
clauses, where pattern matching is used to select one and execute it
while the others that follow are ignored. The consequence is that for
every operation described all the clauses have to be taken into
account.

This also changes the semantics of the final type. The syntax is
similar to that of specs, \textbf{BUT} while in specs overlapping is
not permitted (see Section \ref{sct:dialyzer_preliminaries}), here it
is allowed and the return type of each specific call is calculated by
taking the supremum of the return types of \textbf{EVERY} clause whose
domain overlaps \footnote{This means that the infimum of every
  argument's type in the call and the respective type in the clause is
  not \none} with the inferred types of the arguments in the call.

\subsubsection{Operations}

The operations described in \ref{sct:orig_type_operations} are
modified as follows, with regard to function types:

\begin{description}
\item[Supremum:] If the functions have different arities we maintain
  the old behaviour, collapsing the domains to \any\ and the range to
  the supremum of ranges. If the functions have the same arity, we
  simply add their clauses together to form the supremum. This
  produces an exact supremum. No issue arises in cases of duplicate
  domains as all the clauses are taken into account for further
  calculations (in fact clauses with equal domains are combined into
  one, as described later in this section).
\item[Infimum:] As each function might have more than one clauses,
  infimum is performed per clause. This means that each clause is
  compared against all the clauses of the other function and those who
  have infima that do not have \none\ as an argument or return are
  kept in the result.
\item[Reduction of clauses:] The calculation of supremum and infimum
  is almost certain to produce types that are verbose. This makes the
  clauses list big, requirng both memory to store it and time to
  perform further calculations. An extra step was therefore introduced
  to reduce the number of clauses. Three separate methods of reduction
  are used:
  \begin{enumerate}
    \item \textbf{Combine same domains:} Clauses with the same domain
      should combine their ranges with supremum. An example of this
      technique is given in Example 1 in Listing
      \ref{lst:clauses_reduction_example}.
    \item \textbf{Combine ranges:} Clauses with the same range may
      also be combined if this does not overapproximate the
      domains. This happens when the domains differ in exactly one
      position and the supremum of the types that differ contains only
      the original types (we don't have overapproximations). This
      computation requires a fixpoint termination condition as further
      reduction may be possible in a successive pass. Examples 2 and 3
      in the same Listing illustrate this technique.
    \item \textbf{Remove subclauses:} If both the domain and the range
      of a clause are subtypes of another clause's respective domain
      and range we can remove the clause, as anything using it will
      also use the superclause. This causes intersections to lose
      power in cases where catch-all clauses with return type
      \any\ are present in the code, as these will cause all the rest
      to be absorbed in them. A solution to this issue is proposed in section
      \ref{sct:negative_types}.
      \begin{console}{lst:clauses_reduction_example}{Clauses reduction examples}
        Example 1: Same domains in supremum
        -----------------------------------
        Type A : fun((a) -> b)
        Type B : fun((a) -> c)
        Result : fun((a) -> b; (a) -> c)
        Reduced: fun((a) -> b | c)
        -----------------------------------
        Example 2: Same ranges in supremum
        -----------------------------------
        Type A : fun((a) -> c)
        Type B : fun((b) -> c)
        Result : fun((a) -> c; (b) -> c)
        Reduced: fun((a | b) -> c)
        -----------------------------------
        Example 3: Same ranges, 2 passes
        -----------------------------------
        Initial  : fun((a,c) -> e; (a,d) -> e; (b,c) -> e; (b,d) -> e)
        1st pass : fun((a,c|d) -> e; (b,c|d) -> e)
        2nd pass : fun((a|b,c|d) -> e)

      \end{console}
  \end{enumerate}
\item[Sorting of clauses:] The clauses are sorted according to the
  default ordering of \er's terms both before and after the reductions
  are performed to control both the order of the reductions and the
  final result. This destroys any relation between original clauses in
  the code and resulting success typings but it's important as it
  normalizes the type and maintains the desired property of ``equality
  implies syntactic equality''.
\item[Equality and subtyping:] Equality maintains it's simple, syntax
  based check. The reasons behind this will become clearer after the
  presentation of the changes in the inference algorithm. Clause
  sorting is essential to maintain this property. Subtyping is also
  calculated as described in Section \ref{sct:orig_type_operations}.
\item[Function range:] As described in Section
  \ref{sct:intersection_semantics}, when asking for a function's range
  we may provide information about the argument types and retrieve a
  narrower type.
\end{description}

Other special type functions had to be modified as well to be
compatible with the new representation. What is important to mention
is that in any case where the inner types might be modified (as in
substitutions of opaque types by simple ones and limitation of type
depth), reduction had to be performed as well to ensure the syntactic
equality.

\subsection{Analysis}
\label{sct:intersection_analysis}

The initial steps of the analysis are not changed. Functions are
sorted as described in \ref{sct:orig_analysis} and the success typings
are calculated per SCC. Changes are introduced in both the generation
of constraints and the processing of them to produce the success
typings. As the most important change is implemented in the processing
we will reverse the order of the presentation.

\subsubsection{Changes in constraint processing}
\label{sct:intersect_constr_process}

As we already described in Section \ref{sct:orig_constraints}, each
function has all the constraints organized in a conjunctive list. This
list contains disjunctive lists whenever a branch is present in the
code. Moreover each of these has conjunctive lists with local maps
which are checked for fixpoint and remain unaffected from the other
sides of the branch. Therefore, the simplest and most natural way to
maintain the relation between the various type variables (including
those belonging to the arguments and the result) in each branch is
within the local maps themselves.

Although the values of the type variables are being kept separate in
that way, the type of the function itself is constructed in the main
conjunctive list taking into account the supremum of all the values,
as this is the correct way to handle the types in a disjunctive
list. We need to ``push'' the constraint that binds the type variables
of the arguments and the result with the type variable of the function
into each local map. Taking the \emph{disjunctive normal form} of the
original constraint list accomplishes this goal in a natural way and
separates all the interleavings where nested disjunctive lists are
present (for example a case statement in a branch of a multi-clause
function). An example is provided in Listing \ref{lst:normal_form}. In
this way we generate the correct partial type in every branch and
combine them all in the end using supremum which maintains the
separation. The disjunctive normal form is already used in the
generation of constraints from \emph{guards} to gain precision.
Another benefit we gain is that whenever the function is too
complicated, the calculation of the disjunctive normal form can detect
it and return the original constraint list which when further
processed will return the old-fashioned collapsed type.

\begin{console}{lst:normal_form}{Normal form example}
Conjunctive List 1:
 * var(1) eq fun(var(2)) -> var(3)
 * Disjunctive List 2:
 *  * Conjunctive List 3:
 *  *  * var(2) eq a
 *  *  * var(3) eq b
 *  * Conjunctive List 4:
 *  *  * var(2) sub 1|2
 *  *  * var(4) sub 5|10
 *  *  * var(5) eq var(4)

Disjunctive normal form of the list:

Disjunctive List 1:
 * Conjunctive List 2:
 *  *  var(1) eq fun(var(2)) -> var(3)
 *  *  var(2) eq a
 *  *  var(3) eq b
 *  Conjunctive List 3:
 *  *  var(1) eq fun(var(2)) -> var(3)
 *  *  var(2) sub 1|2
 *  *  var(4) sub 5|10
 *  *  var(5) eq var(4)
\end{console}

\subsubsection{Changes in constraint generation}
\label{sct:intersect_constr_generation}

To gain the benefits of the disjunctive normal form when function
calls are present we need to generate a disjunctive list as a
constraint when processing them. The way this should be done is
obvious: for every clause in the function type a separate conjunctive
list is to be generated, binding the argument and result type
variables to the respective success types in the clause. These
conjunctive lists are then placed in a disjunctive list and the
constraint is ready to be handled by the normalization (see Listing
\ref{lst:call_disjunction} for an example).

\begin{console}{lst:call_disjunction}{Disjunction for function calls}
%%Sample code

bar(1) -> 5;
bar(2) -> 10.

foo(a) -> b;
foo(X) ->
  Y = bar(X),
  Y*X.

%% Supposing we have alredy found the success typing:
%% bar(1) -> 5; (2) -> 10
%% The new constraints for foo are:

Conjunctive List 1:
 * var(1) eq fun(var(2)) -> var(3)
 * Disjunctive List 2:
 *  * Conjunctive List 3:
 *  *  * var(2) eq a
 *  *  * var(3) eq b
 *  * Conjunctive List 4:
 *  *  * Disjunctive List 5:
 *  *  *  *  Conjunctive List 6:
 *  *  *  *  *  var(2) sub 1
 *  *  *  *  *  var(4) sub 5
 *  *  *  *  Conjunctive List 7:
 *  *  *  *  *  var(2) sub 2
 *  *  *  *  *  var(4) sub 10
 *  *  * var(5) eq var(4)
 ...
\end{console}

This is simple in cases where the success typing of the called
function is calculated and fixed, as the generation of the disjunctive
constraint can take place immediately. The hard case is the
self-recursive functions along with those that belong in SCCs. For
these we introduced a new \emph{dynamic constraint} which is to be
substituted before the calculation of the disjunctive normal form by
the disjunctive list derived by the latest success type of the
respective function.

The usage of intersections simplified the generation of constraints
from \emph{contracts} as well, as a similar disjunctive list can be
used for them as well.

\subsubsection{Changes in refinement}
\label{sct:intersect_refinement}

The use of intersectioned types allowed for a small improvement in the
refinement of success typings as well. The previous approach was
\emph{monovariant} in the sense that all the calls to the unexported
functions were found and the types of the actual arguments were
combined in a union that restricted the success typing. This
restriction was taken into account in a new calculation of the success
typing by solving the default constraints with the addition of the
refinement constraint.

Allowing for intersection types, we can keep each call separate and
expect better results from the refinement. This is closer to a
\emph{polyvariant} control flow analysis. The argument types from each
call generate a conjunctive list and all these lists are placed under
a disjunctive list where they can be handled by the normal form
transformation in the usual way.

As an example take the simple reversal of lists, presented in Listing
\ref{lst:reverse}. The one-argument function is exported while the
other is kept local and can therefore be refined. The initial success
typing is very generic because the first clause doesn't restrict the
second argument. Using the union of the types from the two calls we
learn that the second argument is a list, so the result must be a list
as well but no distinction is made. Only by separating the calls and
solving each case separately can we obtain the maximum information
from this code.

\begin{console}{lst:reverse}{Refinement of the success typing of reverse}
reverse(List) ->
  reverse(List, []).

reverse(    [], Acc) -> Acc;
reverse([H| T], Acc) -> reverse(T, [H| Acc]).

%% Initial success typing for reverse/2 is:
-spec reverse([_], _) -> any().

%% Using (_,[_]) as a refinement for the arguments yields:
-spec reverse([_], [_]) -> [_].

-spec reverse([_]) -> [_].

%% Using separate (_,[]) and (_,[_,...]) as a refinement 
%% for the arguments yields:
-spec reverse([_,...],[_]    ) -> [_,...];
             ([]     ,[_,...]) -> [_,...]; 
             ([]     ,[]     ) -> [].

-spec reverse([_,...]) -> [_,...];
             ([]     ) -> [].
\end{console}
